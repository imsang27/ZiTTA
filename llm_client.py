"""
LLM API í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ
Google Gemini API ë˜ëŠ” ì˜¤í”„ë¼ì¸ ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ëª…ë ¹ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
import google.generativeai as genai
from config import Config
import logging

# ë¡œê¹… ì„¤ì • (ë””ë²„ê¹…ìš©)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OfflineLLM:
    """ì˜¤í”„ë¼ì¸ ëª¨ë“œ LLM (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ)"""
    
    def __init__(self):
        """ì˜¤í”„ë¼ì¸ LLM ì´ˆê¸°í™”"""
        self.responses = {
            "ì¸ì‚¬": ["ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ZiTTAì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?", "ë°˜ê°‘ìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ë„ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!"],
            "ë‚ ì”¨": ["ì£„ì†¡í•˜ì§€ë§Œ ì˜¤í”„ë¼ì¸ ëª¨ë“œì—ì„œëŠ” ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
            "ì‹œê°„": ["í˜„ì¬ ì‹œê°„ì„ í™•ì¸í•˜ë ¤ë©´ ì‹œìŠ¤í…œ ì‹œê³„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."],
        }
    
    def generate_response(self, user_message: str) -> str:
        """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        message_lower = user_message.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ
        if any(word in message_lower for word in ["ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "ë°˜ê°€"]):
            import random
            return random.choice(self.responses["ì¸ì‚¬"])
        elif any(word in message_lower for word in ["ë‚ ì”¨", "ê¸°ì˜¨", "ì˜¨ë„"]):
            return self.responses["ë‚ ì”¨"][0]
        elif any(word in message_lower for word in ["ì‹œê°„", "ëª‡ ì‹œ"]):
            from datetime import datetime
            return f"í˜„ì¬ ì‹œê°„ì€ {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}ì…ë‹ˆë‹¤."
        else:
            return "ì˜¤í”„ë¼ì¸ ëª¨ë“œì—ì„œëŠ” ì œí•œì ì¸ ì‘ë‹µë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì˜¨ë¼ì¸ ëª¨ë“œë¡œ ì „í™˜í•˜ì‹œë©´ ë” ë§ì€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

class LLMClient:
    """LLM í´ë¼ì´ì–¸íŠ¸ (ì˜¨ë¼ì¸/ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì§€ì›)"""
    
    def __init__(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.use_offline = Config.USE_OFFLINE_MODE
        
        if self.use_offline:
            # ì˜¤í”„ë¼ì¸ ëª¨ë“œ
            self.offline_llm = OfflineLLM()
            self.model = None
            self.chat_session = None
            print("ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        else:
            # ì˜¨ë¼ì¸ ëª¨ë“œ (Gemini API)
            if not Config.GEMINI_API_KEY:
                raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            genai.configure(api_key=Config.GEMINI_API_KEY)
            
            # ëª¨ë¸ ì´ˆê¸°í™” ì‹œë„
            try:
                # Generation configë¥¼ ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ì„¤ì •
                self.generation_config = genai.types.GenerationConfig(
                    temperature=Config.LLM_TEMPERATURE
                )
                self.model = genai.GenerativeModel(
                    Config.LLM_MODEL,
                    generation_config=self.generation_config
                )
            except Exception as e:
                # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                available_models = self._get_available_models()
                error_msg = f"ëª¨ë¸ '{Config.LLM_MODEL}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                error_msg += f"ì˜¤ë¥˜: {str(e)}\n\n"
                if available_models:
                    error_msg += "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:\n"
                    for model in available_models:
                        error_msg += f"  - {model}\n"
                else:
                    error_msg += "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                raise ValueError(error_msg)
            
            self.temperature = Config.LLM_TEMPERATURE
            self.offline_llm = None
            
            # ì±„íŒ… ì„¸ì…˜ ì´ˆê¸°í™”
            self.chat_session = None
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """ë‹¹ì‹ ì€ ZiTTAì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê°œì¸ AI ë¹„ì„œë¡œì„œ ë˜‘ë˜‘í•˜ë©´ì„œë„ ìœ ë¨¸ëŸ¬ìŠ¤í•œ ëŒ€í™”ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ëª…ë ¹ì„ ì´í•´í•˜ê³  ì ì ˆíˆ ì‘ë‹µí•˜ì„¸ìš”. í•  ì¼ ê´€ë¦¬, ë©”ëª¨, íŒŒì¼ íƒìƒ‰ ë“±ì˜ ì‘ì—…ì„ ë„ì™€ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    
    def _get_available_models(self) -> list:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Returns:
            ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        try:
            models = genai.list_models()
            available_models = []
            for model in models:
                # GENERATE_CONTENTë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ í•„í„°ë§
                if 'generateContent' in model.supported_generation_methods:
                    available_models.append(model.name.replace('models/', ''))
            return available_models
        except Exception as e:
            print(f"ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def chat(self, user_message: str, conversation_history: list = None) -> str:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            conversation_history: ëŒ€í™” ê¸°ë¡ (ì„ íƒì )
            
        Returns:
            LLM ì‘ë‹µ ë¬¸ìì—´
        """
        if self.use_offline:
            # ì˜¤í”„ë¼ì¸ ëª¨ë“œ
            return self.offline_llm.generate_response(user_message)
        
        # ì˜¨ë¼ì¸ ëª¨ë“œ (Gemini API)
        try:
            # ì±„íŒ… ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ëœ ê²½ìš° ìƒˆ ì„¸ì…˜ ì‹œì‘
            if self.chat_session is None or not conversation_history:
                self.chat_session = self.model.start_chat(history=[])
                # ì²« ë©”ì‹œì§€ì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨
                initial_prompt = f"{self.system_prompt}\n\nì‚¬ìš©ì: {user_message}"
                # generation_configëŠ” ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ì„¤ì •ë˜ë¯€ë¡œ ë³„ë„ë¡œ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
                response = self.chat_session.send_message(initial_prompt)
            else:
                # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ì´ ìˆëŠ” ê²½ìš°, Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                # conversation_historyëŠ” OpenAI í˜•ì‹ì´ë¯€ë¡œ Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”
                # í•˜ì§€ë§Œ GeminiëŠ” ìë™ìœ¼ë¡œ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ ë‹¨ìˆœíˆ ë©”ì‹œì§€ë§Œ ì „ì†¡
                response = self.chat_session.send_message(user_message)
            
            # ì‘ë‹µ ì²˜ë¦¬ - Gemini APIëŠ” response.textë¡œ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥
            if response is None:
                logger.error("API ì‘ë‹µì´ Noneì…ë‹ˆë‹¤")
                return "ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
            # response.text ì†ì„±ìœ¼ë¡œ ì§ì ‘ ì ‘ê·¼ ì‹œë„ (ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•)
            try:
                if hasattr(response, 'text'):
                    response_text = response.text
                    logger.info(f"response.textë¡œ ì‘ë‹µ ë°›ìŒ: ê¸¸ì´={len(response_text) if response_text else 0}")
                    if response_text and response_text.strip():
                        return response_text.strip()
                    else:
                        logger.warning("response.textê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            except Exception as text_error:
                logger.warning(f"response.text ì ‘ê·¼ ì‹¤íŒ¨: {text_error}")
                # text ì†ì„± ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                pass
            
            # response.textê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ ë°©ë²• ì‹œë„
            # candidatesë¥¼ í†µí•´ ì ‘ê·¼
            try:
                if hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content'):
                            content = candidate.content
                            if hasattr(content, 'parts') and content.parts:
                                text_parts = []
                                for part in content.parts:
                                    if hasattr(part, 'text') and part.text:
                                        text_parts.append(part.text)
                                if text_parts:
                                    return ''.join(text_parts).strip()
            except Exception as candidate_error:
                pass
            
            # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ - ì‹¤ì œ ì‘ë‹µ ê°ì²´ ì •ë³´ë¥¼ í¬í•¨í•œ ë””ë²„ê¹… ë©”ì‹œì§€
            response_info = f"ì‘ë‹µ íƒ€ì…: {type(response).__name__}"
            if hasattr(response, '__dict__'):
                response_info += f", ì†ì„±: {list(response.__dict__.keys())}"
            elif hasattr(response, '__class__'):
                response_info += f", ë©”ì„œë“œ: {[m for m in dir(response) if not m.startswith('_')][:10]}"
            
            return f"ì‘ë‹µì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {response_info}"
            
        except Exception as e:
            error_str = str(e)
            error_type = type(e).__name__
            logger.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_type} - {error_str}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
            if "429" in error_str or "quota" in error_str.lower() or "exceeded" in error_str.lower():
                error_msg = self._format_quota_error_message(error_str)
                return error_msg
            # ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ì¸ì§€ í™•ì¸
            elif "not found" in error_str.lower() or "404" in error_str or "not supported" in error_str.lower():
                available_models = self._get_available_models()
                error_msg = self._format_model_error_message(error_str, available_models)
                return error_msg
            else:
                # ê¸°íƒ€ ì˜¤ë¥˜ - ê°„ë‹¨í•œ ë©”ì‹œì§€ë§Œ ë°˜í™˜
                return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({error_type}): {error_str}"
    
    def _format_model_error_message(self, error_str: str, available_models: list) -> str:
        """
        ëª¨ë¸ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê°€ë…ì„± ì¢‹ê²Œ í¬ë§·íŒ…
        
        Args:
            error_str: ì˜¤ë¥˜ ë©”ì‹œì§€
            available_models: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
            
        Returns:
            í¬ë§·íŒ…ëœ ì˜¤ë¥˜ ë©”ì‹œì§€
        """
        # ì¶”ì²œ ëª¨ë¸ (ì•ˆì •ì ì´ê³  ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸)
        # ê°€ì´ë“œëŠ” ì£¼ë¡œ gemini-2.5-flash / gemini-2.5-flash-lite ì‚¬ìš©ì„ ê¶Œì¥
        recommended_models = [
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.5-pro",
            "gemini-flash-latest",
            "gemini-pro-latest",
        ]
        
        # ëª¨ë¸ ê·¸ë£¹í™”
        recommended = []
        gemini_models = []
        gemma_models = []
        preview_models = []
        other_models = []
        
        for model in available_models:
            if model in recommended_models:
                recommended.append(model)
            elif model.startswith("gemini-"):
                if "preview" in model.lower() or "exp" in model.lower():
                    preview_models.append(model)
                else:
                    gemini_models.append(model)
            elif model.startswith("gemma-"):
                gemma_models.append(model)
            else:
                other_models.append(model)
        
        # ê°„ë‹¨í•œ HTML í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (ê¸°ë³¸ íƒœê·¸ë§Œ ì‚¬ìš©)
        error_msg = f"""<b>âŒ ëª¨ë¸ ì˜¤ë¥˜</b><br>
<pre style="color: #666; font-size: 0.9em; white-space: pre-wrap;">{error_str}</pre>
<br>
<b>ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡</b><br><br>"""
        
        # ì¶”ì²œ ëª¨ë¸ ì„¹ì…˜
        if recommended:
            error_msg += f"""<b>â­ ì¶”ì²œ ëª¨ë¸ (ì•ˆì •ì )</b><br>"""
            for model in sorted(recommended):
                error_msg += f"""&nbsp;&nbsp;&nbsp;&nbsp;â€¢ {model}<br>"""
            error_msg += "<br>"
        
        # Gemini ì¼ë°˜ ëª¨ë¸
        if gemini_models:
            error_msg += f"""<b>ğŸ¤– Gemini ëª¨ë¸</b><br>"""
            for model in sorted(gemini_models):
                error_msg += f"""&nbsp;&nbsp;&nbsp;&nbsp;â€¢ {model}<br>"""
            error_msg += "<br>"
        
        # Gemma ëª¨ë¸
        if gemma_models:
            error_msg += f"""<b>ğŸ’ Gemma ëª¨ë¸</b><br>"""
            for model in sorted(gemma_models):
                error_msg += f"""&nbsp;&nbsp;&nbsp;&nbsp;â€¢ {model}<br>"""
            error_msg += "<br>"
        
        # Preview/Experimental ëª¨ë¸
        if preview_models:
            error_msg += f"""<b>ğŸ”¬ Preview/Experimental ëª¨ë¸</b><br>"""
            for model in sorted(preview_models):
                error_msg += f"""&nbsp;&nbsp;&nbsp;&nbsp;â€¢ {model}<br>"""
            error_msg += "<br>"
        
        # ê¸°íƒ€ ëª¨ë¸
        if other_models:
            error_msg += f"""<b>ğŸ“¦ ê¸°íƒ€ ëª¨ë¸</b><br>"""
            for model in sorted(other_models):
                error_msg += f"""&nbsp;&nbsp;&nbsp;&nbsp;â€¢ {model}<br>"""
            error_msg += "<br>"
        
        error_msg += f"""<br>
<b>ğŸ’¡ í•´ê²° ë°©ë²•</b><br>
&nbsp;&nbsp;&nbsp;&nbsp;1. .env íŒŒì¼ì„ ì—´ì–´ì£¼ì„¸ìš”<br>
&nbsp;&nbsp;&nbsp;&nbsp;2. LLM_MODEL ê°’ì„ ìœ„ ëª©ë¡ ì¤‘ í•˜ë‚˜ë¡œ ë³€ê²½í•˜ì„¸ìš”<br>
&nbsp;&nbsp;&nbsp;&nbsp;3. ì¶”ì²œ: <code>gemini-2.5-flash</code> ë˜ëŠ” <code>gemini-2.5-flash-lite</code><br>
&nbsp;&nbsp;&nbsp;&nbsp;4. í˜„ì¬ ì„¤ì •: <b>{Config.LLM_MODEL}</b>"""
        
        if not available_models:
            error_msg = f"""<b>âŒ ëª¨ë¸ ì˜¤ë¥˜</b><br>
<pre style="color: #666; white-space: pre-wrap;">{error_str}</pre>
<br>
<b>âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</b><br>
API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."""
        
        return error_msg
    
    def _format_quota_error_message(self, error_str: str) -> str:
        """
        í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ê°€ë…ì„± ì¢‹ê²Œ í¬ë§·íŒ…
        
        Args:
            error_str: ì˜¤ë¥˜ ë©”ì‹œì§€
            
        Returns:
            í¬ë§·íŒ…ëœ ì˜¤ë¥˜ ë©”ì‹œì§€
        """
        import re
        
        # ì¬ì‹œë„ ì‹œê°„ ì¶”ì¶œ
        retry_match = re.search(r'Please retry in ([\d.]+)s', error_str)
        retry_time = retry_match.group(1) if retry_match else None
        
        # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
        model_match = re.search(r'model: ([a-z0-9-]+)', error_str)
        model_name = model_match.group(1) if model_match else Config.LLM_MODEL
        
        error_msg = f"""<b>âš ï¸ API í• ë‹¹ëŸ‰ ì´ˆê³¼</b><br><br>
<b>ë¬¸ì œ:</b> Gemini APIì˜ ë¬´ë£Œ í‹°ì–´ í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.<br><br>
<b>í˜„ì¬ ëª¨ë¸:</b> {model_name}<br>"""
        
        if retry_time:
            minutes = int(float(retry_time) // 60)
            seconds = int(float(retry_time) % 60)
            if minutes > 0:
                retry_text = f"{minutes}ë¶„ {seconds}ì´ˆ"
            else:
                retry_text = f"{seconds}ì´ˆ"
            error_msg += f"""<b>ì¬ì‹œë„ ê°€ëŠ¥ ì‹œê°„:</b> ì•½ {retry_text} í›„<br><br>"""
        
        error_msg += f"""<b>ğŸ’¡ í•´ê²° ë°©ë²•:</b><br>
&nbsp;&nbsp;&nbsp;&nbsp;1. <b>ì ì‹œ ê¸°ë‹¤ë¦¬ê¸°:</b> í• ë‹¹ëŸ‰ì´ ë¦¬ì…‹ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì„¸ìš” (ë³´í†µ 1ë¶„ ë˜ëŠ” 1ì¼ ë‹¨ìœ„)<br>
&nbsp;&nbsp;&nbsp;&nbsp;2. <b>ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©:</b> í• ë‹¹ëŸ‰ì´ ë” ë§ì€ ëª¨ë¸ë¡œ ë³€ê²½í•˜ì„¸ìš”<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ì¶”ì²œ: <code>gemini-2.5-flash</code> ë˜ëŠ” <code>gemini-2.5-flash-lite</code><br>
&nbsp;&nbsp;&nbsp;&nbsp;3. <b>ìœ ë£Œ í”Œëœìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ:</b> ë” ë†’ì€ í• ë‹¹ëŸ‰ì„ ì‚¬ìš©í•˜ë ¤ë©´ Google AI Studioì—ì„œ í”Œëœì„ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”<br><br>
<b>ğŸ“š ìì„¸í•œ ì •ë³´:</b><br>
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ í• ë‹¹ëŸ‰ ì •ë³´: <a href="https://ai.google.dev/gemini-api/docs/rate-limits">https://ai.google.dev/gemini-api/docs/rate-limits</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ ì‚¬ìš©ëŸ‰ í™•ì¸: <a href="https://ai.dev/usage?tab=rate-limit">https://ai.dev/usage?tab=rate-limit</a>"""
        
        return error_msg
    
    def process_command(self, command: str, context: dict = None) -> dict:
        """
        ìì—°ì–´ ëª…ë ¹ì„ ì²˜ë¦¬í•˜ê³  ì ì ˆí•œ ì‘ì—…ì„ ìˆ˜í–‰
        
        Args:
            command: ì‚¬ìš©ì ëª…ë ¹
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ê°„ë‹¨í•œ ëª…ë ¹ ì¸ì‹ (í–¥í›„ í™•ì¥ ê°€ëŠ¥)
        command_lower = command.lower()
        
        # í•  ì¼ ê´€ë ¨ ëª…ë ¹
        if any(keyword in command_lower for keyword in ["í•  ì¼", "todo", "í•´ì•¼", "í•´ì•¼ í• "]):
            return {
                "type": "todo",
                "action": "create" if any(k in command_lower for k in ["ì¶”ê°€", "ë§Œë“¤", "ìƒì„±"]) else "list",
                "command": command
            }
        
        # ë©”ëª¨ ê´€ë ¨ ëª…ë ¹
        if any(keyword in command_lower for keyword in ["ë©”ëª¨", "memo", "ë…¸íŠ¸", "note"]):
            return {
                "type": "memo",
                "action": "create" if any(k in command_lower for k in ["ì¶”ê°€", "ë§Œë“¤", "ìƒì„±", "ì‘ì„±"]) else "list",
                "command": command
            }
        
        # íŒŒì¼ ê´€ë ¨ ëª…ë ¹
        if any(keyword in command_lower for keyword in ["íŒŒì¼", "file", "í´ë”", "folder", "ë””ë ‰í† ë¦¬"]):
            return {
                "type": "file",
                "action": "list",
                "command": command
            }
        
        # ì¼ë°˜ ëŒ€í™”
        return {
            "type": "chat",
            "action": "respond",
            "command": command
        }

